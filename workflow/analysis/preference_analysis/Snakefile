# Snakemake workflow for News Citation Preference Analysis
#
# This workflow analyzes how different patterns of news citations affect
# user preferences in AI search responses using a Bradley-Terry model approach.

import os
from pathlib import Path

# Configuration
configfile: "config/config.yaml"

# Define paths from config
INPUT_DIR = Path(config["cleaned_data_dir"])
OUTPUT_DIR = Path(config["analysis_dir"])
SCRIPTS_DIR = Path(workflow.basedir) / "scripts"

# Ensure output directory exists
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

# Final outputs - includes all analysis components
rule all:
    input:
        OUTPUT_DIR / "preference_analysis_report.html",
        OUTPUT_DIR / "visualizations" / "effect_sizes.png",
        OUTPUT_DIR / "visualizations" / "model_comparison.png",
        OUTPUT_DIR / "bt_ratings_results.json",
        OUTPUT_DIR / "individual_effects_results.json",
        OUTPUT_DIR / "citation_style_effects_results.json"

# Phase 1: Data Preparation
rule prepare_data:
    input:
        citations = INPUT_DIR / "citations_enriched.parquet",
        responses = INPUT_DIR / "responses.parquet",
        threads = INPUT_DIR / "threads.parquet"
    output:
        threads = OUTPUT_DIR / "news_competitions.parquet",
        responses = OUTPUT_DIR / "news_competitions_responses.parquet",
        citations = OUTPUT_DIR / "news_competitions_citations.parquet"
    script:
        "scripts/prepare_news_competitions.py"

# Phase 2: Response Signal Computation
rule compute_response_signals:
    input:
        threads = OUTPUT_DIR / "news_competitions.parquet",
        responses = OUTPUT_DIR / "news_competitions_responses.parquet",
        citations = OUTPUT_DIR / "news_competitions_citations.parquet"
    output:
        OUTPUT_DIR / "news_competitions_response_signals.parquet"
    script:
        "scripts/compute_response_signals.py"

# Phase 3: Battle Data Creation
rule create_battles:
    input:
        threads = OUTPUT_DIR / "news_competitions.parquet",
        signals = OUTPUT_DIR / "news_competitions_response_signals.parquet"
    output:
        OUTPUT_DIR / "battle_data.parquet"
    script:
        "scripts/create_battle_data.py"

# Phase 4a: Bradley-Terry Ratings
rule compute_bt_ratings:
    input:
        OUTPUT_DIR / "battle_data.parquet"
    output:
        results = OUTPUT_DIR / "bt_ratings_results.json",
        coefficients = OUTPUT_DIR / "bt_ratings_coefficients.csv"
    script:
        "scripts/bt_ratings.py"

# Phase 4b: Individual Feature Effects
rule analyze_individual_effects:
    input:
        OUTPUT_DIR / "battle_data.parquet"
    output:
        results = OUTPUT_DIR / "individual_effects_results.json",
        coefficients = OUTPUT_DIR / "individual_effects_coefficients.csv"
    params:
        bootstrap_samples = config["statistical_analysis"]["bootstrap_samples"] // 2,  # Fewer samples for individual analysis
        random_seed = config["statistical_analysis"]["random_seed"]
    script:
        "scripts/individual_effects.py"

# Phase 4c: Citation Style Effects with Flexible Models
rule analyze_citation_style_effects:
    input:
        OUTPUT_DIR / "battle_data.parquet"
    output:
        results = OUTPUT_DIR / "citation_style_effects_results.json",
        coefficients = OUTPUT_DIR / "citation_style_effects_coefficients.csv"
    params:
        bootstrap_samples = config["statistical_analysis"]["bootstrap_samples"],
        random_seed = config["statistical_analysis"]["random_seed"],
        selected_models = ["basic_response", "political_bias", "source_quality", "full_primary"]  # Can be customized
    script:
        "scripts/citation_style_effects.py"

# Flexible citation style analysis with custom model specifications
rule analyze_citation_style_custom:
    input:
        OUTPUT_DIR / "battle_data.parquet"
    output:
        results = OUTPUT_DIR / "citation_style_effects_{model_set}_results.json",
        coefficients = OUTPUT_DIR / "citation_style_effects_{model_set}_coefficients.csv"
    params:
        bootstrap_samples = config["statistical_analysis"]["bootstrap_samples"],
        random_seed = config["statistical_analysis"]["random_seed"],
        # Selected models will be determined by the {model_set} wildcard
        # Examples: "basic", "political", "quality", "extended", "all"
        selected_models = lambda wildcards: {
            "basic": ["basic_response"],
            "political": ["political_bias", "source_types"],
            "quality": ["source_quality", "politics_and_quality"],
            "extended": ["extended_news"],
            "all": None,  # Run all available models
            "core": ["basic_response", "political_bias", "source_quality"],
            "advanced": ["politics_and_quality", "full_primary", "extended_news"]
        }.get(wildcards.model_set, ["basic_response"])
    script:
        "scripts/citation_style_effects.py"


# Phase 5: Reporting and Visualization
rule generate_report:
    input:
        # All analysis results
        bt_ratings_results = OUTPUT_DIR / "bt_ratings_results.json",
        individual_effects_results = OUTPUT_DIR / "individual_effects_results.json",
        citation_style_effects_results = OUTPUT_DIR / "citation_style_effects_results.json",
        # All coefficients
        bt_ratings_coefficients = OUTPUT_DIR / "bt_ratings_coefficients.csv",
        individual_effects_coefficients = OUTPUT_DIR / "individual_effects_coefficients.csv",
        citation_style_effects_coefficients = OUTPUT_DIR / "citation_style_effects_coefficients.csv",
        # Battle data for context
        battle_data = OUTPUT_DIR / "battle_data.parquet"
    output:
        report = OUTPUT_DIR / "preference_analysis_report.html",
        effect_sizes = OUTPUT_DIR / "visualizations" / "effect_sizes.png",
        confidence_intervals = OUTPUT_DIR / "visualizations" / "confidence_intervals.png",
        significance = OUTPUT_DIR / "visualizations" / "statistical_significance.png",
        model_comparison = OUTPUT_DIR / "visualizations" / "model_comparison.png"
    script:
        "scripts/generate_report.py"

# Convenience rules for running partial workflows

# Data preparation stages
rule prepare_only:
    input:
        OUTPUT_DIR / "news_competitions.parquet"

rule compute_signals_only:
    input:
        OUTPUT_DIR / "news_competitions_response_signals.parquet"

rule create_battles_only:
    input:
        OUTPUT_DIR / "battle_data.parquet"

# Individual analysis components
rule bt_ratings_only:
    input:
        OUTPUT_DIR / "bt_ratings_results.json"

rule individual_effects_only:
    input:
        OUTPUT_DIR / "individual_effects_results.json"

rule citation_style_effects_only:
    input:
        OUTPUT_DIR / "citation_style_effects_results.json"


rule report_only:
    input:
        OUTPUT_DIR / "preference_analysis_report.html"

# All analysis components (without report)
rule all_analyses:
    input:
        OUTPUT_DIR / "bt_ratings_results.json",
        OUTPUT_DIR / "individual_effects_results.json",
        OUTPUT_DIR / "citation_style_effects_results.json"

# Clean intermediate files
rule clean:
    shell:
        """
        rm -rf {OUTPUT_DIR}/news_*.parquet
        rm -rf {OUTPUT_DIR}/response_signals.parquet
        rm -rf {OUTPUT_DIR}/battle_data.parquet
        rm -rf {OUTPUT_DIR}/logs/
        """

# Clean all outputs
rule clean_all:
    shell:
        """
        rm -rf {OUTPUT_DIR}/*
        """